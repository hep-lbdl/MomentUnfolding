{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import matplotlib.pyplot as plt\n",
    "import energyflow as ef\n",
    "import energyflow.archs\n",
    "from energyflow.archs import PFN\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.lines as mlines\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import corner\n",
    "\n",
    "\n",
    "plt.rc('font', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng()\n",
    "n_dim = 2\n",
    "gauss_data = rng.multivariate_normal(np.zeros(n_dim), np.identity(n_dim), 100000)\n",
    "gauss_sim = rng.multivariate_normal(np.ones(n_dim), np.identity(n_dim), 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self._lambda0 = self.add_weight(name='lambda0', \n",
    "                                    shape=(n_dim, 1),\n",
    "                                    initializer = tf.keras.initializers.RandomUniform(minval=-5., maxval=5.), \n",
    "                                    trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        #return tf.exp(self._lambda1 * x + self._lambda0)\n",
    "        return tf.exp(x @ self._lambda0)\n",
    "\n",
    "mymodel_inputtest = Input(shape=(n_dim,))\n",
    "mymodel_test = MyLayer()(mymodel_inputtest)\n",
    "model_generator = Model(mymodel_inputtest, mymodel_test)\n",
    "\n",
    "inputs_disc = Input((n_dim, ))\n",
    "hidden_layer_1_disc = Dense(50, activation='relu')(inputs_disc)\n",
    "hidden_layer_2_disc = Dense(50, activation='relu')(hidden_layer_1_disc)\n",
    "hidden_layer_3_disc = Dense(50, activation='relu')(hidden_layer_2_disc)\n",
    "outputs_disc = Dense(1, activation='sigmoid')(hidden_layer_3_disc)\n",
    "model_discrimantor = Model(inputs=inputs_disc, outputs=outputs_disc)\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = tf.gather(y_true, [1], axis=1) # event weights\n",
    "    y_true = tf.gather(y_true, [0], axis=1) # actual y_true for loss\n",
    "    \n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = -weights * ((y_true) * K.log(y_pred)/weights_1 +\n",
    "                         (1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "\n",
    "model_discrimantor.compile(loss=weighted_binary_crossentropy, optimizer='adam')\n",
    "\n",
    "def weighted_binary_crossentropy_GAN(y_true, y_pred):\n",
    "    weights = tf.gather(y_pred, [1], axis=1) # event weights\n",
    "    y_pred = tf.gather(y_pred, [0], axis=1) # actual y_pred for loss\n",
    "    \n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "    \n",
    "    #tf.print(\"weights\",weights_0,weights_1)\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = weights * ((1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "    \n",
    "model_discrimantor.trainable = False\n",
    "mymodel_gan = Input(shape=(n_dim,))\n",
    "gan_model = Model(inputs=mymodel_gan,outputs=concatenate([model_discrimantor(mymodel_gan),model_generator(mymodel_gan)]))\n",
    "\n",
    "gan_model.compile(loss=weighted_binary_crossentropy_GAN, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch = 0 [[[-1.2789392]\n",
      "  [ 3.8647003]]]\n",
      "on epoch = 10 [[[-1.3335301]\n",
      "  [ 3.7667277]]]\n",
      "on epoch = 20 [[[-1.3738631]\n",
      "  [ 3.6364532]]]\n",
      "on epoch = 30 [[[-1.4080834]\n",
      "  [ 3.5056667]]]\n",
      "on epoch = 40 [[[-1.4378322]\n",
      "  [ 3.3702831]]]\n",
      "on epoch = 50 [[[-1.4625871]\n",
      "  [ 3.2300935]]]\n",
      "on epoch = 60 [[[-1.478641 ]\n",
      "  [ 3.0858252]]]\n",
      "on epoch = 70 [[[-1.4821802]\n",
      "  [ 2.93733  ]]]\n",
      "on epoch = 80 [[[-1.4703212]\n",
      "  [ 2.7841988]]]\n",
      "on epoch = 90 [[[-1.4413182]\n",
      "  [ 2.626203 ]]]\n"
     ]
    }
   ],
   "source": [
    "xvals_1 = np.concatenate([gauss_data,gauss_sim])\n",
    "yvals_1 = np.concatenate([np.ones(len(gauss_data)),np.zeros(len(gauss_sim))])\n",
    "\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(xvals_1, yvals_1)\n",
    "\n",
    "b = 100\n",
    "n_epochs = b\n",
    "n_batch = 128*b\n",
    "n_batches = len(X_train_1) // n_batch\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    mylambda0 = model_generator.layers[-1].get_weights()\n",
    "    if i%(b//10)==0:\n",
    "        print(\"on epoch =\", i, np.array(mylambda0))\n",
    "    #print(\"  \",np.sum(model_generator.predict(X_train_1,batch_size=1000)))\n",
    "    for j in range(n_batches):\n",
    "        X_batch = X_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        Y_batch = Y_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        W_batch = model_generator(X_batch)\n",
    "        W_batch = np.array(W_batch).flatten()\n",
    "        W_batch[Y_batch==1] = 1\n",
    "        #W_batch[Y_batch==0] = 1\n",
    "        \n",
    "        Y_batch_2 = np.stack((Y_batch, W_batch), axis=1)\n",
    "        \n",
    "        model_discrimantor.train_on_batch(X_batch, Y_batch_2)\n",
    "        \n",
    "        #print(\"      \",j,np.sum(model_generator.predict(X_batch,batch_size=1000)),np.log(model_generator.predict([1.]))-np.log(model_generator.predict([0.])),np.log(model_generator.predict([0.])))\n",
    "        \n",
    "        gan_model.train_on_batch(X_batch[Y_batch==0],np.zeros(len(X_batch[Y_batch==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data [0.00838812 0.00728141]\n",
      "Sim [1.010666   0.99926261]\n",
      "Weighted Sim [-0.97098688  3.63861248]\n"
     ]
    }
   ],
   "source": [
    "lambda0 = np.array(model_generator.layers[-1].get_weights()).reshape(n_dim)\n",
    "weights_1 = np.concatenate([np.ones(len(gauss_data)),np.exp(gauss_sim @ lambda0)*len(gauss_data)/np.sum(np.exp(gauss_sim @ lambda0))])\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1, w_train_1, w_test_1 = train_test_split(xvals_1, yvals_1, weights_1)\n",
    "\n",
    "print(\"Data\", np.mean(X_test_1[Y_test_1==1], axis=0))\n",
    "print(\"Sim\", np.mean(X_test_1[Y_test_1==0], axis=0))\n",
    "print(\"Weighted Sim\", np.average(X_test_1[Y_test_1==0],weights=w_test_1[Y_test_1==0], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self._lambda0 = self.add_weight(name='lambda0', \n",
    "                                    shape=(n_dim, 1),\n",
    "                                    initializer = tf.keras.initializers.Constant(0.1),\n",
    "                                        #tf.keras.initializers.RandomUniform(minval=-5., maxval=5.), \n",
    "                                    trainable=True)\n",
    "        self._lambda1 = self.add_weight(name='lambda1', \n",
    "                            shape=(n_dim, n_dim),\n",
    "                            initializer = tf.keras.initializers.Constant(0.1),\n",
    "                                        #tf.keras.initializers.RandomUniform(minval=-5., maxval=5.), \n",
    "                            trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "#         print(f\"{x.shape = }\")\n",
    "#         print(f\"{self._lambda0.shape = }\")\n",
    "#         print(f\"{self._lambda1.shape = }\")\n",
    "        Term1 = x @ self._lambda0\n",
    "        Term2 = tf.linalg.diag_part(x @ self._lambda1 @ tf.transpose(x))\n",
    "        Term2 = tf.expand_dims(Term2, -1)\n",
    "#         print(f\"{Term1.shape = }\")\n",
    "#         print(f\"{Term2.shape = }\")\n",
    "#         print(f\"{(Term1 + Term2).shape = }\")\n",
    "        \n",
    "        return tf.exp(Term1 + Term2)\n",
    "\n",
    "mymodel_inputtest = Input(shape=(n_dim,))\n",
    "mymodel_test = MyLayer()(mymodel_inputtest)\n",
    "model_generator = Model(mymodel_inputtest, mymodel_test)\n",
    "\n",
    "inputs_disc = Input((n_dim, ))\n",
    "hidden_layer_1_disc = Dense(50, activation='relu')(inputs_disc)\n",
    "hidden_layer_2_disc = Dense(50, activation='relu')(hidden_layer_1_disc)\n",
    "hidden_layer_3_disc = Dense(50, activation='relu')(hidden_layer_2_disc)\n",
    "outputs_disc = Dense(1, activation='sigmoid')(hidden_layer_3_disc)\n",
    "model_discrimantor = Model(inputs=inputs_disc, outputs=outputs_disc)\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = tf.gather(y_true, [1], axis=1) # event weights\n",
    "    y_true = tf.gather(y_true, [0], axis=1) # actual y_true for loss\n",
    "    \n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = -weights * ((y_true) * K.log(y_pred)/weights_1 +\n",
    "                         (1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "\n",
    "model_discrimantor.compile(loss=weighted_binary_crossentropy, optimizer='adam')\n",
    "\n",
    "def weighted_binary_crossentropy_GAN(y_true, y_pred):\n",
    "    weights = tf.gather(y_pred, [1], axis=1) # event weights\n",
    "    y_pred = tf.gather(y_pred, [0], axis=1) # actual y_pred for loss\n",
    "    \n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "    \n",
    "    #tf.print(\"weights\",weights_0,weights_1)\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = weights * ((1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "    \n",
    "model_discrimantor.trainable = False\n",
    "mymodel_gan = Input(shape=(n_dim,))\n",
    "gan_model = Model(inputs=mymodel_gan,outputs=concatenate([model_discrimantor(mymodel_gan),model_generator(mymodel_gan)]))\n",
    "\n",
    "gan_model.compile(loss=weighted_binary_crossentropy_GAN, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch = 0\n",
      "[[0.1]\n",
      " [0.1]\n",
      " [0.1]]\n",
      "[[0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1]]\n",
      "WARNING:tensorflow:5 out of the last 2201 calls to <function Model.make_train_function.<locals>.train_function at 0x7f892c5f6430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "on epoch = 10\n",
      "[[-0.03484944]\n",
      " [-0.03237605]\n",
      " [-0.03469536]]\n",
      "[[-0.00916653 -0.01971382 -0.01996004]\n",
      " [-0.01971382 -0.01057663 -0.0175357 ]\n",
      " [-0.01996004 -0.01753569 -0.01512844]]\n",
      "on epoch = 20\n",
      "[[-0.18040502]\n",
      " [-0.17839366]\n",
      " [-0.17955777]]\n",
      "[[-0.10332684 -0.11964564 -0.12141013]\n",
      " [-0.11964563 -0.10747249 -0.11440329]\n",
      " [-0.1214101  -0.11440329 -0.11044312]]\n",
      "on epoch = 30\n",
      "[[-0.3046459 ]\n",
      " [-0.30497426]\n",
      " [-0.30403447]]\n",
      "[[-0.17687069 -0.18710117 -0.19073603]\n",
      " [-0.18710114 -0.17764111 -0.18283243]\n",
      " [-0.190736   -0.1828324  -0.18066981]]\n",
      "on epoch = 40\n",
      "[[-0.41755444]\n",
      " [-0.4201348 ]\n",
      " [-0.4177858 ]]\n",
      "[[-0.23552282 -0.23705713 -0.24261133]\n",
      " [-0.2370571  -0.2304167  -0.23569284]\n",
      " [-0.2426113  -0.23569281 -0.23234229]]\n",
      "on epoch = 50\n",
      "[[-0.52409476]\n",
      " [-0.5293728 ]\n",
      " [-0.52599657]]\n",
      "[[-0.28312063 -0.27498236 -0.28266442]\n",
      " [-0.27498233 -0.2699781  -0.27788344]\n",
      " [-0.2826644  -0.27788338 -0.26927266]]\n",
      "on epoch = 60\n",
      "[[-0.6266938 ]\n",
      " [-0.63563174]\n",
      " [-0.6317535 ]]\n",
      "[[-0.3220467  -0.30336532 -0.31313318]\n",
      " [-0.3033653  -0.29781914 -0.31142715]\n",
      " [-0.31313315 -0.3114271  -0.2931623 ]]\n",
      "on epoch = 70\n",
      "[[-0.7267581]\n",
      " [-0.7407757]\n",
      " [-0.7372287]]\n",
      "[[-0.35327482 -0.32339886 -0.33494595]\n",
      " [-0.32339883 -0.31413934 -0.33674306]\n",
      " [-0.33494592 -0.336743   -0.30413458]]\n",
      "on epoch = 80\n",
      "[[-0.82514817]\n",
      " [-0.84658426]\n",
      " [-0.84415907]]\n",
      "[[-0.3774683  -0.33546463 -0.3481321 ]\n",
      " [-0.3354646  -0.31767008 -0.35323432]\n",
      " [-0.34813207 -0.35323426 -0.30053788]]\n",
      "on epoch = 90\n",
      "[[-0.922184  ]\n",
      " [-0.95462424]\n",
      " [-0.95502645]]\n",
      "[[-0.3952961  -0.33960924 -0.3519018 ]\n",
      " [-0.3396092  -0.30583218 -0.35856086]\n",
      " [-0.35190177 -0.3585608  -0.2780399 ]]\n"
     ]
    }
   ],
   "source": [
    "xvals_1 = np.concatenate([gauss_data,gauss_sim])\n",
    "yvals_1 = np.concatenate([np.ones(len(gauss_data)),np.zeros(len(gauss_sim))])\n",
    "\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(xvals_1, yvals_1)\n",
    "\n",
    "b = 100\n",
    "n_epochs = b\n",
    "n_batch = 128*b\n",
    "n_batches = len(X_train_1) // n_batch\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    mylambda0 = model_generator.layers[-1].get_weights()[0]\n",
    "    mylambda1 = model_generator.layers[-1].get_weights()[1]\n",
    "    if i%(b//10)==0:\n",
    "        print(\"on epoch =\", i)\n",
    "        print(np.array(mylambda0))\n",
    "        print(np.array(mylambda1))\n",
    "    #print(\"  \",np.sum(model_generator.predict(X_train_1,batch_size=1000)))\n",
    "    for j in range(n_batches):\n",
    "        X_batch = X_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        #print(f\"{X_batch.shape = }\")\n",
    "        Y_batch = Y_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        #print(f\"{Y_batch.shape = }\")\n",
    "        W_batch = model_generator(X_batch)\n",
    "        #print(f\"Pre flattening {W_batch.shape = }\")\n",
    "        W_batch = np.array(W_batch).flatten()\n",
    "        #print(f\"Post flattening {W_batch.shape = }\")\n",
    "        W_batch[Y_batch==1] = 1\n",
    "        W_batch[Y_batch==0] = 1\n",
    "        \n",
    "        Y_batch_2 = np.stack((Y_batch, W_batch), axis=1)\n",
    "        \n",
    "        model_discrimantor.train_on_batch(X_batch, Y_batch_2)\n",
    "        \n",
    "        #print(\"      \",j,np.sum(model_generator.predict(X_batch,batch_size=1000)),np.log(model_generator.predict([1.]))-np.log(model_generator.predict([0.])),np.log(model_generator.predict([0.])))\n",
    "        \n",
    "        gan_model.train_on_batch(X_batch[Y_batch==0],np.zeros(len(X_batch[Y_batch==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda0 = np.array(model_generator.layers[-1].get_weights()[0]).reshape(n_dim)\n",
    "lambda1 = np.array(model_generator.layers[-1].get_weights()[1]).reshape((n_dim, n_dim))\n",
    "A = (gauss_sim @ lambda1 @ gauss_sim.T).diagonal()\n",
    "weights_1 = np.concatenate([np.ones(len(gauss_data)),np.exp(gauss_sim @ lambda0 + A)*len(gauss_data)/np.sum(np.exp(gauss_sim @ lambda0 + A))])\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1, w_train_1, w_test_1 = train_test_split(xvals_1, yvals_1, weights_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      "Data [-0.0121952  -0.00516054 -0.00771615]\n",
      "Sim [0.99839323 0.9957826  1.00157077]\n",
      "Weighted Sim [ 0.00305805 -0.00293258 -0.0696174 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean:\")\n",
    "print(\"Data\", np.mean(X_test_1[Y_test_1==1], axis=0))\n",
    "print(\"Sim\", np.mean(X_test_1[Y_test_1==0], axis=0))\n",
    "print(\"Weighted Sim\", np.average(X_test_1[Y_test_1==0],weights=w_test_1[Y_test_1==0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Variance:\n",
      "Data [[ 1.   -0.01  0.  ]\n",
      " [-0.01  1.   -0.  ]\n",
      " [ 0.   -0.    1.  ]]\n",
      "Sim [[2.01 0.99 1.01]\n",
      " [0.99 1.98 1.  ]\n",
      " [1.01 1.   2.02]]\n",
      "Weighted Sim [[ 0.7  -0.21 -0.23]\n",
      " [-0.21  0.86 -0.3 ]\n",
      " [-0.23 -0.3   0.97]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\\nVariance:\")\n",
    "print(\"Data\", np.round(np.mean([[X_test_1[Y_test_1==1][:, i]*X_test_1[Y_test_1==1][:, j] for i in range(3)] for j in range(3)], axis=-1), 2))\n",
    "print(\"Sim\", np.round(np.mean([[X_test_1[Y_test_1==0][:, i]*X_test_1[Y_test_1==0][:, j] for i in range(3)] for j in range(3)], axis=-1), 2))\n",
    "print(\"Weighted Sim\", np.round(np.average([[X_test_1[Y_test_1==0][:, i]*X_test_1[Y_test_1==0][:, j] for i in range(3)] for j in range(3)], axis=-1,weights=w_test_1[Y_test_1==0]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
