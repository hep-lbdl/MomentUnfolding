{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import energyflow as ef\n",
    "import energyflow.archs\n",
    "from energyflow.archs import PFN\n",
    "from matplotlib import gridspec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, concatenate, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import zscore\n",
    "from keras import backend\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = True\n",
    "data_size = 10**6\n",
    "mylambda = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "datasets = {'Pythia26': ef.zjets_delphes.load('Pythia26', num_data=data_size),\n",
    "            'Herwig': ef.zjets_delphes.load('Herwig', num_data=data_size)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gen = datasets['Pythia26']['gen_widths'] #gen = particle level\n",
    "w_sim = datasets['Pythia26']['sim_widths'] #sim = detector level\n",
    "w_truth = datasets['Herwig']['gen_widths']\n",
    "w_data = datasets['Herwig']['sim_widths']\n",
    "\n",
    "p_gen = datasets['Pythia26']['gen_jets'][:,0] #gen = particle level\n",
    "p_sim = datasets['Pythia26']['sim_jets'][:,0] #sim = detector level\n",
    "p_truth = datasets['Herwig']['gen_jets'][:,0]\n",
    "p_data = datasets['Herwig']['sim_jets'][:,0]\n",
    "\n",
    "\n",
    "if normalize:   \n",
    "    wm = np.mean(w_truth)\n",
    "    ws = np.std(w_truth)\n",
    "    pm = np.mean(p_truth)\n",
    "    ps = np.mean(p_truth)\n",
    "    \n",
    "    w_gen = (w_gen - wm)/ws\n",
    "    w_sim = (w_sim - wm)/ws\n",
    "    w_truth = (w_truth - wm)/ws\n",
    "    w_data = (w_data - wm)/ws\n",
    "    \n",
    "    p_gen = (p_gen - pm)/ps\n",
    "    p_sim = (p_sim - pm)/ps\n",
    "    p_truth = (p_truth - pm)/ps\n",
    "    p_data = (p_data - pm)/ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = tf.gather(y_true, [1], axis=1) # event weights\n",
    "    y_true = tf.gather(y_true, [0], axis=1) # actual y_true for loss\n",
    "\n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "\n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = -weights * ((y_true) * K.log(y_pred)/weights_1 +\n",
    "                         (1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "\n",
    "def weighted_binary_crossentropy_GAN(y_true, y_pred):\n",
    "    weights = tf.gather(y_pred, [1], axis=1) # event weights\n",
    "    y_pred = tf.gather(y_pred, [0], axis=1) # actual y_pred for loss\n",
    "\n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "\n",
    "    #tf.print(\"weights\",weights_0,weights_1)\n",
    "\n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = weights * ((1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable(package='Custom', name='entropy')\n",
    "class EntropyRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, beta=0.):\n",
    "        self.beta = beta\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return -1*self.beta * tf.math.reduce_sum(-1*x*tf.math.log(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'beta': float(self.beta)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel_inputtest = Input(shape=(1,))\n",
    "hidden_layer_1 = Dense(50, activation='ReLU',use_bias=False)(mymodel_inputtest)\n",
    "batch_norm_1 = BatchNormalization()(hidden_layer_1)\n",
    "hidden_layer_2 = Dense(50, activation='ReLU',use_bias=False)(batch_norm_1)\n",
    "batch_norm_2 = BatchNormalization()(hidden_layer_2)\n",
    "hidden_layer_3 = Dense(50, activation='ReLU',use_bias=False)(batch_norm_2)\n",
    "outputs = tf.exp(Dense(1, activation='sigmoid',use_bias=False,\n",
    "                      kernel_regularizer=EntropyRegularizer(beta = 0.5))(hidden_layer_3))\n",
    "model_generator = Model(mymodel_inputtest, outputs)\n",
    "\n",
    "inputs_disc = Input((1, ))\n",
    "hidden_layer_1_disc = Dense(50, activation='relu')(inputs_disc)\n",
    "hidden_layer_2_disc = Dense(50, activation='relu')(hidden_layer_1_disc)\n",
    "hidden_layer_3_disc = Dense(50, activation='relu')(hidden_layer_2_disc)\n",
    "outputs_disc = Dense(1, activation='sigmoid')(hidden_layer_3_disc)\n",
    "model_discrimintor = Model(inputs=inputs_disc, outputs=outputs_disc)\n",
    "\n",
    "model_discrimintor.compile(loss=weighted_binary_crossentropy, optimizer='adam')\n",
    "\n",
    "model_discrimintor.trainable = False\n",
    "mymodel_gan = Input(shape=(1,))\n",
    "gan_model = Model(inputs=mymodel_gan,outputs=concatenate([model_discrimintor(mymodel_gan),model_generator(mymodel_gan)]))\n",
    "\n",
    "\n",
    "gan_model.compile(loss=weighted_binary_crossentropy_GAN, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals_particle = np.concatenate([w_gen,w_truth])\n",
    "xvals_detector = np.concatenate([w_sim,w_data])     \n",
    "yvals = np.transpose(np.concatenate([np.ones(len(w_gen)),np.zeros(len(w_truth))]))\n",
    "\n",
    "X_train_particle, X_test_particle, X_train_detector, X_test_detector, Y_train, Y_test = train_test_split(xvals_particle, \n",
    "                                                                                                        xvals_detector,\n",
    "                                                                                                        yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch= 0 1.6476638 1.6285193 1.6495969\n",
      "on epoch= 1 nan nan nan\n",
      "on epoch= 2 nan nan nan\n",
      "on epoch= 3 nan nan nan\n",
      "on epoch= 4 nan nan nan\n",
      "on epoch= 5 nan nan nan\n",
      "on epoch= 6 nan nan nan\n",
      "on epoch= 7 nan nan nan\n",
      "on epoch= 8 nan nan nan\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "n_batch = 128*10\n",
    "n_batches = len(X_train_particle) // n_batch\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    mypreds = model_generator.predict(X_test_particle,batch_size=1000)\n",
    "    print(\"on epoch=\",i,np.mean(mypreds),np.min(mypreds),np.max(mypreds))\n",
    "    for j in range(n_batches):\n",
    "        X_batch_particle = X_train_particle[j*n_batch:(j+1)*n_batch]\n",
    "        X_batch_detector = X_train_detector[j*n_batch:(j+1)*n_batch]\n",
    "        Y_batch = Y_train[j*n_batch:(j+1)*n_batch]\n",
    "        W_batch = model_generator(X_batch_particle)\n",
    "        W_batch = np.array(W_batch).flatten()\n",
    "\n",
    "        W_batch[Y_batch==1] = 1        \n",
    "        Y_batch_2 = np.stack((Y_batch, W_batch), axis=1)\n",
    "\n",
    "        model_discrimintor.train_on_batch(X_batch_detector, Y_batch_2)        \n",
    "        gan_model.train_on_batch(X_batch_particle[Y_batch==0],Y_batch[Y_batch==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_generator.predict(X_test_particle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bins = np.linspace(0,1,30)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "nbins = 30\n",
    "bins = np.linspace(X_test_particle.min(),X_test_particle.max(),nbins)\n",
    "\n",
    "_,_,_=plt.hist(X_test_particle[Y_test==0],bins=bins,alpha=0.5,label=\"truth\",density=True, zorder=-1)\n",
    "_,_,_=plt.hist(X_test_particle[Y_test==1],bins=bins,alpha=0.5,label=\"gen\",density=True, zorder=0)\n",
    "_,_,_=plt.hist(X_test_particle[Y_test==0],bins=bins,weights=weights[Y_test==0],histtype=\"step\",color=\"red\",ls=\":\", lw=2,label=\"weighted gen\",density=True, zorder=1)\n",
    "plt.legend(fontsize=15)\n",
    "plt.ylabel(\"Trials\")\n",
    "plt.xlabel(\"jet width\")\n",
    "#plt.savefig(\"Unifold.pdf\", bbox_inches='tight', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
