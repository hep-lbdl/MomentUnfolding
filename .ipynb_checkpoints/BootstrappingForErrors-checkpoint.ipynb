{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a34e4-d1be-47cb-be66-b17299ff6dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "plt.rcParams[\"font.family\"] = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ba685-9c5b-4739-a754-7f8c532e0d40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalize = True\n",
    "N = 10**6\n",
    "n_moments = 2\n",
    "mylambda = []\n",
    "n_bootstraps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a1089-fb34-49db-a4f3-b7dc636d0cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moment = 2\n",
    "obs = 'm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5c6730-ba4a-4c77-b3b5-d466902eef38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load and normalize the data\n",
    "data = np.load('MomentUnfoldingFinal/npfiles/rawdata.npz')\n",
    "substructure_variables = ['pT', 'w', 'q', 'm', 'r', 'tau1s', 'tau2s']\n",
    "data_streams = ['_true', '_true_alt', '_reco', '_reco_alt']\n",
    "n_variables = len(substructure_variables)\n",
    "\n",
    "\n",
    "normalize = False\n",
    "    \n",
    "for var_name in data.files:\n",
    "    globals()[var_name] = data[var_name]\n",
    "    \n",
    "if normalize:\n",
    "    for var_name in substructure_variables:\n",
    "        mu = np.mean(globals()[var_name+data_streams[0]])\n",
    "        sig = np.std(globals()[var_name + data_streams[0]])\n",
    "        for stream in data_streams:\n",
    "            globals()[var_name+stream] = (globals()[var_name+stream] - mu)/sig\n",
    "            \n",
    "for stream in data_streams:\n",
    "    globals()[f\"x{stream}\"] = globals()[obs + stream]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5116f2-8781-458b-88f5-440b4fff7c61",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a366fa-3354-40d1-bb7a-37ac48de27e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfeb43a-da82-4a24-8b13-ae860e6316ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_true = rng.normal(0,1, N)\n",
    "x_reco = rng.normal(x_true, 0.5)\n",
    "x_true_alt = rng.normal(-0.5,1,N)\n",
    "x_reco_alt = rng.normal(x_true_alt, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132780d2-c3c0-4b65-bbd1-9bb245771369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xvals = np.concatenate([x_true_alt,x_true])\n",
    "xvals_reco = np.concatenate([x_reco_alt,x_reco])\n",
    "yvals = np.concatenate([np.ones(len(x_true_alt)),np.zeros(len(x_true))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c996d0c-f97b-461c-9346-7c771369bf87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomUniform(minval=-.5, maxval=.5)\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, myc, **kwargs):\n",
    "        self.myinit = myc\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self._l = self.add_weight(name='l', \n",
    "                                    shape=(n_moments,),\n",
    "                                    initializer=tf.keras.initializers.Constant(self.myinit), \n",
    "                                    trainable=True)\n",
    "        \n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.exp(sum([self._l[i]* x**(i+1) for i in range(n_moments)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a73eb0-460a-474b-a578-a1decba76ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = tf.gather(y_true, [1], axis=1) # event weights\n",
    "    y_true = tf.gather(y_true, [0], axis=1) # actual y_true for loss\n",
    "\n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "\n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = -weights * ((y_true) * K.log(y_pred)/weights_1 +\n",
    "                         (1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "\n",
    "def weighted_binary_crossentropy_GAN(y_true, y_pred):\n",
    "    weights = tf.gather(y_pred, [1], axis=1) # event weights\n",
    "    y_pred = tf.gather(y_pred, [0], axis=1) # actual y_pred for loss\n",
    "\n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "\n",
    "    #tf.print(\"weights\",weights_0,weights_1)\n",
    "\n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = weights * ((1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7c421-68f2-4f01-a3da-c92887d2d214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for btsrp in range(n_bootstraps):\n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb72565-59eb-4b0e-8440-7a0578fe906f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "initializer = tf.keras.initializers.RandomUniform(minval=-5., maxval=5.)\n",
    "\n",
    "class MyLayer(Layer):\n",
    "\n",
    "    def __init__(self, myc, **kwargs):\n",
    "        self.myinit = myc\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self._lambda0 = self.add_weight(name='lambda0', \n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Constant(self.myinit), \n",
    "                                    trainable=True)\n",
    "        self._lambda1 = self.add_weight(name='lambda1', \n",
    "                                    shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Constant(self.myinit), \n",
    "                                    trainable=True)\n",
    "        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        #return tf.exp(self._lambda1 * x + self._lambda0)\n",
    "        return tf.exp(self._lambda0 * x + self._lambda1 * x**2)\n",
    "\n",
    "myc = 0.1\n",
    "mymodel_inputtest = Input(shape=(1,))\n",
    "mymodel_test = MyLayer(myc)(mymodel_inputtest)\n",
    "model_generator = Model(mymodel_inputtest, mymodel_test)\n",
    "\n",
    "inputs_disc = Input((1, ))\n",
    "hidden_layer_1_disc = Dense(50, activation='relu')(inputs_disc)\n",
    "hidden_layer_2_disc = Dense(50, activation='relu')(hidden_layer_1_disc)\n",
    "hidden_layer_3_disc = Dense(50, activation='relu')(hidden_layer_2_disc)\n",
    "outputs_disc = Dense(1, activation='sigmoid')(hidden_layer_3_disc)\n",
    "model_discrimantor = Model(inputs=inputs_disc, outputs=outputs_disc)\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = tf.gather(y_true, [1], axis=1) # event weights\n",
    "    y_true = tf.gather(y_true, [0], axis=1) # actual y_true for loss\n",
    "    \n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = -weights * ((y_true) * K.log(y_pred)/weights_1 +\n",
    "                         (1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "\n",
    "model_discrimantor.compile(loss=weighted_binary_crossentropy, optimizer='adam')\n",
    "\n",
    "def weighted_binary_crossentropy_GAN(y_true, y_pred):\n",
    "    weights = tf.gather(y_pred, [1], axis=1) # event weights\n",
    "    y_pred = tf.gather(y_pred, [0], axis=1) # actual y_pred for loss\n",
    "    \n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "    \n",
    "    #tf.print(\"weights\",weights_0,weights_1)\n",
    "    \n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = weights * ((1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "    \n",
    "model_discrimantor.trainable = False\n",
    "mymodel_gan = Input(shape=(1,))\n",
    "gan_model = Model(inputs=mymodel_gan,outputs=concatenate([model_discrimantor(mymodel_gan),model_generator(mymodel_gan)]))\n",
    "\n",
    "gan_model.compile(loss=weighted_binary_crossentropy_GAN, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902e915-2b01-48d5-bc11-91e85853addc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#xvals_1 = np.concatenate([gauss_data,gauss_sim])\n",
    "#yvals_1 = np.concatenate([np.ones(len(gauss_data)),np.zeros(len(gauss_sim))])\n",
    "\n",
    "xvals_1 = np.concatenate([x_true_alt,x_true])\n",
    "yvals_1 = np.concatenate([np.ones(len(x_true_alt)),np.zeros(len(x_true))])\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(xvals_1, yvals_1)\n",
    "\n",
    "n_epochs = 20\n",
    "n_batch = 128*100\n",
    "n_batches = len(X_train_1) // n_batch\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    lambdasum = np.log(model_generator.predict([1.]))\n",
    "    lambdasum2 = np.log(model_generator.predict([2.]))\n",
    "    mylambda1 = (lambdasum2-2*lambdasum)/2\n",
    "    mylambda0 = lambdasum - mylambda1\n",
    "    print(\"on epoch=\",i,mylambda0,mylambda1)\n",
    "    #print(\"  \",np.sum(model_generator.predict(X_train_1,batch_size=1000)))\n",
    "    for j in range(n_batches):\n",
    "        X_batch = X_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        Y_batch = Y_train_1[j*n_batch:(j+1)*n_batch]\n",
    "        W_batch = model_generator(X_batch)\n",
    "        W_batch = np.array(W_batch).flatten()\n",
    "        W_batch[Y_batch==1] = 1\n",
    "        #W_batch[Y_batch==0] = 1\n",
    "        \n",
    "        Y_batch_2 = np.stack((Y_batch, W_batch), axis=1)\n",
    "        \n",
    "        model_discrimantor.train_on_batch(X_batch, Y_batch_2)\n",
    "        \n",
    "        #print(\"      \",j,np.sum(model_generator.predict(X_batch,batch_size=1000)),np.log(model_generator.predict([1.]))-np.log(model_generator.predict([0.])),np.log(model_generator.predict([0.])))\n",
    "        \n",
    "        gan_model.train_on_batch(X_batch[Y_batch==0],np.zeros(len(X_batch[Y_batch==0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4f6f3-cebe-4b58-b1c4-8ec0020301a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.exp(np.sum([mylambda[-1][:, k]*x['gen']**(k+1) for k in range(n_moments)],axis = 0))\n",
    "weights = np.concatenate([arr*len(x['gen'])/np.sum(arr), np.ones(len(x['gen']))])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, w_train, w_test = train_test_split(xvals, yvals, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac10851-ae55-414e-a50d-af979991c77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.0",
   "language": "python",
   "name": "tensorflow-2.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
