{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import energyflow as ef\n",
    "import energyflow.archs\n",
    "from energyflow.archs import PFN\n",
    "from matplotlib import gridspec\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Layer, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "plt.rc('font', size=20)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" # pick a number < 4 on ML4HEP\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "#These are the same datasets from the OmniFold paper https://arxiv.org/abs/1911.09107.  More detail at https://energyflow.network/docs/datasets/.\n",
    "#Pythia and Herwig are two generators; one will be treated here as the \"simulation\" and one as \"data\".\n",
    "datasets = {'Pythia26': ef.zjets_delphes.load('Pythia26', num_data=1000000),\n",
    "            'Herwig': ef.zjets_delphes.load('Herwig', num_data=1000000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true = datasets['Pythia26']['gen_widths'] #gen = particle level\n",
    "w_reco = datasets['Pythia26']['sim_widths'] #sim = detector level\n",
    "w_true_alt = datasets['Herwig']['gen_widths']\n",
    "w_reco_alt = datasets['Herwig']['sim_widths']\n",
    "\n",
    "p_gen = datasets['Pythia26']['gen_jets'][:,0] #gen = particle level\n",
    "p_sim = datasets['Pythia26']['sim_jets'][:,0] #sim = detector level\n",
    "p_truth = datasets['Herwig']['gen_jets'][:,0]\n",
    "p_data = datasets['Herwig']['sim_jets'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = tf.gather(y_true, [1], axis=1) # event weights\n",
    "    y_true = tf.gather(y_true, [0], axis=1) # actual y_true for loss\n",
    "\n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "\n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = -weights * ((y_true) * K.log(y_pred)/weights_1 +\n",
    "                         (1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)\n",
    "\n",
    "def weighted_binary_crossentropy_GAN(y_true, y_pred):\n",
    "    weights = tf.gather(y_pred, [1], axis=1) # event weights\n",
    "    y_pred = tf.gather(y_pred, [0], axis=1) # actual y_pred for loss\n",
    "\n",
    "    weights_1 = K.sum(y_true*weights)\n",
    "    weights_0 = K.sum((1-y_true)*weights)\n",
    "\n",
    "    #tf.print(\"weights\",weights_0,weights_1)\n",
    "\n",
    "    # Clip the prediction value to prevent NaN's and Inf's\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    t_loss = weights * ((1 - y_true) * K.log(1 - y_pred)/weights_0)\n",
    "    return K.mean(t_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals_1 = np.concatenate([w_true_alt,w_true])\n",
    "yvals_1 = np.concatenate([np.ones(len(w_true_alt)),np.zeros(len(w_true))])\n",
    "\n",
    "xvals_2 = np.concatenate([w_reco_alt,w_reco])\n",
    "yvals_2 = np.concatenate([np.ones(len(w_reco_alt)),np.zeros(len(w_reco))])\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1, X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(xvals_1, \n",
    "                                                                                    yvals_1, xvals_2, yvals_2)\n",
    "\n",
    "myc = 0.1\n",
    "mymodel_inputtest = Input(shape=(1,))\n",
    "\n",
    "\n",
    "    \n",
    "while n < N:\n",
    "    print(f\"{n = }\")\n",
    "    \n",
    "    mymodel_test = MyLayer(myc)(mymodel_inputtest)\n",
    "    model_generator = Model(mymodel_inputtest, mymodel_test)\n",
    "\n",
    "    inputs_disc = Input((1, ))\n",
    "    hidden_layer_1_disc = Dense(50, activation='relu')(inputs_disc)\n",
    "    hidden_layer_2_disc = Dense(50, activation='relu')(hidden_layer_1_disc)\n",
    "    hidden_layer_3_disc = Dense(50, activation='relu')(hidden_layer_2_disc)\n",
    "    outputs_disc = Dense(1, activation='sigmoid')(hidden_layer_3_disc)\n",
    "    model_discrimantor = Model(inputs=inputs_disc, outputs=outputs_disc)\n",
    "\n",
    "    model_discrimantor.compile(loss=weighted_binary_crossentropy, optimizer='adam')\n",
    "\n",
    "    model_discrimantor.trainable = False\n",
    "    mymodel_gan = Input(shape=(1,))\n",
    "    gan_model = Model(inputs=mymodel_gan,outputs=concatenate([model_discrimantor(mymodel_gan),model_generator(mymodel_gan)]))\n",
    "\n",
    "\n",
    "    gan_model.compile(loss=weighted_binary_crossentropy_GAN, optimizer='adam')\n",
    "\n",
    "    n_epochs = 10\n",
    "    n_batch = 128*10\n",
    "    n_batches = len(X_train_1) // n_batch\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(n_batches):\n",
    "            X_batch = X_train_1[j*n_batch:(j+1)*n_batch]\n",
    "            Y_batch = Y_train_1[j*n_batch:(j+1)*n_batch]\n",
    "            W_batch = model_generator(X_batch)\n",
    "            W_batch = np.array(W_batch).flatten()\n",
    "            W_batch[Y_batch==1] = 1\n",
    "            #W_batch[Y_batch==0] = 1\n",
    "\n",
    "            Y_batch_2 = np.stack((Y_batch, W_batch), axis=1)\n",
    "\n",
    "            model_discrimantor.train_on_batch(X_batch, Y_batch_2)\n",
    "\n",
    "            gan_model.train_on_batch(X_batch[Y_batch==0],np.zeros(len(X_batch[Y_batch==0])))\n",
    "            \n",
    "        mylambda = np.array(model_generator.layers[-1].get_weights())\n",
    "        print(\"on epoch=\",i, mylambda)\n",
    "\n",
    "    arr = np.array([mylambda[:, k]*w_true**(k+1) for k in range(n)])\n",
    "    exponent = np.exp(np.sum(arr, axis=0))\n",
    "    weights_1 = np.concatenate([np.ones(len(w_true_alt)),exponent*len(w_true_alt)/np.sum(exponent)])\n",
    "\n",
    "    X_train_1, X_test_1, Y_train_1, Y_test_1, X_train_2, X_test_2, Y_train_2, Y_test_2, w_train_1, w_test_1 = train_test_split(xvals_1, \n",
    "                                                                                    yvals_1, xvals_2, yvals_2, weights_1)\n",
    "\n",
    "    #data, simulation w/o weights, weighted simulation\n",
    "    for i in range(n):\n",
    "        print(\"Moment \", i+1)\n",
    "        print(np.mean(X_test_2[Y_test_2==1]**(i+1)),np.mean(X_test_2[Y_test_2==0]**(i+1)),np.average(X_test_2[Y_test_2==0]**(i+1),weights=w_test_1[Y_test_2==0]))\n",
    "    errors_weighted[:, n] = [(np.average(X_test_2[Y_test_2==0]**(i+1),weights=w_test_1[Y_test_2==0]) - np.mean(X_test_2[Y_test_2==1]**(i+1)))/np.average(X_test_2[Y_test_2==0]**(i+1),weights=w_test_1[Y_test_2==0]) for i in range(N)]\n",
    "    errors_unweighted[:, n] = [(np.mean(X_test_2[Y_test_2==0]**(i+1)) - np.mean(X_test_2[Y_test_2==1]**(i+1)))/np.mean(X_test_2[Y_test_2==0]**(i+1)) for i in range(N)]\n",
    "    n += 1\n",
    "    print(\"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
